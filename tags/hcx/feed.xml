<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>HCX on vUptime.io - Cloud builder(s)</title><link>https://vuptime.io/tags/hcx/</link><description>Recent content in HCX on vUptime.io - Cloud builder(s)</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Ludovic Rivallain and blog co-authors</copyright><lastBuildDate>Wed, 26 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://vuptime.io/tags/hcx/feed.xml" rel="self" type="application/rss+xml"/><item><title>How to retain your IP addresses while migrating to Azure VMware Solution</title><link>https://vuptime.io/post/2025-03-26-keep-ip-addresses-while-migrating-to-avs/</link><pubDate>Wed, 26 Mar 2025 00:00:00 +0000</pubDate><guid>https://vuptime.io/post/2025-03-26-keep-ip-addresses-while-migrating-to-avs/</guid><description>
&lt;p>Migrating On Premises assets to a cloud solution can be a complex process, especially when it comes to considering existing IP addresses plan. One key benefit of migrating to Azure VMware Solution (AVS) is the ability to retain existing IP addresses, which can simplify the migration process and reduce downtime. However, this approach requires careful planning and consideration of network principles to ensure a smooth transition.&lt;/p>
&lt;p>This post will explore some considerations for retaining IP addresses during the migration to AVS, including the importance of understanding network dependencies and the potential impact on performance. It will also discuss the benefits of leveraging VMware Hybrid Cloud Extension (HCX) Layer 2 extensions to facilitate the migration process.&lt;/p>
&lt;h2 id="general-considerations">General Considerations&lt;/h2>
&lt;p>First and foremost, we need to align ourselves on some fundamental network principles before exploring solutions. These principles are crucial to understand when planning a migration to Azure VMware Solution (AVS) and maintaining existing IP addresses, as we cannot re-invent networking principles to suit our IP journey expectations.&lt;/p>
&lt;h3 id="a-single-routed-network-can-only-be-accessible-from-one-location">A single routed network can only be accessible from one location&lt;/h3>
&lt;p>It seems obvious, but it is important to understand that a routed network can only be accessible from one location at a time. This means that if you have a routed network in your on-premises environment (like 10.1.2.0/24 for example), a network with the same IP address range cannot be published from the cloud side at the same time. This is a fundamental principle of networking and must be taken into account when planning your migration.&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="A single routed network can only be accessible from one location"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/avs-keep-ip/a-single-routed-network-can-only-be-accessible-from-one-location.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;h3 id="assets-in-a-single-vlan-share-the-same-broadcast-domain">Assets in a single vLAN share the same broadcast domain&lt;/h3>
&lt;p>When assets are in the same VLAN, they share the same broadcast domain. This means that they can communicate with each other without the need for a router. This will also applied on assets in an extended L2 network.&lt;/p>
&lt;p>Example of ARP broadcast over an extended L2 network:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Assets in a single vLAN share the same broadcast domain"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/avs-keep-ip/assets-in-a-single-vlan-subnet-share-the-same-broadcast-domain.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;h3 id="migrated-asset-will-continue-to-use-its-configured-gateway-to-reach-routed-peers">Migrated asset will continue to use its configured gateway to reach routed peers&lt;/h3>
&lt;p>When you migrate an asset to the cloud and retain its IP address, it will continue to use its configured gateway to reach routed peers. This means that if you have a routed network in your on-premises environment, the migrated asset will still use its on-premises gateway to communicate with other assets in the same network. Typically, this connectivity path will be utilized for both egress and ingress traffic.&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="If retaining IP address: migrated asset will continue to use its configured gateway to reach routed peers"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/avs-keep-ip/migrated-asset-will-still-continue-to-use-its-configured-gateway-to-reach-routed-peers.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;h3 id="latency-will-exist-between-migrated-and-non-migrated-assets">Latency will exist between migrated and non-migrated assets&lt;/h3>
&lt;p>When you migrate an asset to the cloud and retain its IP address, latency will exist between migrated and non-migrated assets. In addition to the familiar on-premises connectivity, several other factors will contribute to latency:&lt;/p>
&lt;ul>
&lt;li>Geographical distance between the two locations&lt;/li>
&lt;li>Link quality and routing hops&lt;/li>
&lt;li>Link usage and congestion&lt;/li>
&lt;li>Tunneling techniques and protocols used&lt;/li>
&lt;li>Possible tromboning of traffic&lt;/li>
&lt;/ul>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Latency will exist between migrated and non-migrated assets"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/avs-keep-ip/latency-will-exist-between-migrated-and-non-migrated-assets.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>Traffic latency can also affect cloud to cloud communication in case of extended L2 networks: this is the tromboning effect.&lt;/p>
&lt;h4 id="what-is-traffic-tromboning">What is traffic tromboning?&lt;/h4>
&lt;p>Traffic tromboning occurs when traffic is sent from one location to another and then back again, rather than taking a direct path.&lt;/p>
&lt;p>&lt;strong>Example:&lt;/strong> One of the worst scenarios is when a migrated asset in the cloud tries to communicate with another asset in a different network. If the gateway of one asset is on-premises, the traffic will travel from the migrated asset (cloud side) to the on-premises gateway, and then back to the cloud side to reach the destination asset. The response will follow the same path: from the destination asset (cloud side) to the on-premises gateway, and finally back to the migrated asset.&lt;/p>
&lt;p>This pattern can strongly affect performance and perceived latency.&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="What is traffic tromboning?"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/avs-keep-ip/traffic-tromboning.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;h2 id="extend-network-to-retain-ip-addresses">Extend network to retain IP addresses&lt;/h2>
&lt;p>As you may already have guessed, the solution to retain IP addresses while migrating to a cloud solution, is to extend the network(s) and to consider migration &amp;quot;per network&amp;quot; instead of per VM/application or other kind of asset.&lt;/p>
&lt;p>In order to do things properly, I will recommend the following approach:&lt;/p>
&lt;h3 id="execution-plan">Execution Plan&lt;/h3>
&lt;ol>
&lt;li>&lt;strong>Plan, plan, plan!&lt;/strong>
&lt;ul>
&lt;li>Carefully select network(s) to extend.&lt;/li>
&lt;li>Understand the next phases and the way to execute them with the network to extend.&lt;/li>
&lt;li>Understand all the dependencies of the network.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Extend L2 Network&lt;/strong>
&lt;ul>
&lt;li>Network will now be able to host assets in two locations.&lt;/li>
&lt;li>The gateway will remain on-premises (for most assets).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Migrate Assets&lt;/strong>
&lt;ul>
&lt;li>Migrated assets will retain connectivity and IP addresses.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Evacuate Remaining Assets&lt;/strong>
&lt;ul>
&lt;li>If needed: some assets may require reIP to ensure the network is free from resources on-premises.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Switchover Connectivity&lt;/strong>
&lt;ul>
&lt;li>Connectivity is now switched to the cloud side.&lt;/li>
&lt;li>All workloads will use native connectivity.&lt;/li>
&lt;li>L2 Extension is removed.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>Regarding the migration project, each step of the execution plan has its own criticality level, which can be summarized as follows:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Phase&lt;/th>
&lt;th>Criticality&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Plan&lt;/td>
&lt;td>&lt;span style="color: #ff5733;">Critical&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Extend L2 Network&lt;/td>
&lt;td>&lt;span style="color: #33e0ff;">Low&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Migrate Assets&lt;/td>
&lt;td>&lt;span style="color: #ff9f33;">Medium&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Evacuate Remaining Assets&lt;/td>
&lt;td>&lt;span style="color: #33e0ff;">Low&lt;/span>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Switchover Connectivity&lt;/td>
&lt;td>&lt;span style="color: #ff5733;">Critical&lt;/span>*&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>* &lt;em>Depends a lot on the planning phase.&lt;/em>&lt;/p>
&lt;h3 id="how-does-it-work">How does it work?&lt;/h3>
&lt;p>Here is a simplified diagram of how the network extension works:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="How does it work?"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/avs-keep-ip/how-does-it-work.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>After carreful planning:&lt;/p>
&lt;ol>
&lt;li>The network extension is implemented using extension technology (examples will be provided later).&lt;/li>
&lt;li>Assets from this network are either migrated (retaining their IP addresses) or evacuated from the network (Re-IP, decommission, etc.).&lt;/li>
&lt;li>The network extension is removed, and connectivity is switched to the cloud side.&lt;/li>
&lt;/ol>
&lt;h3 id="how-is-my-network-currently-built">How is my network currently built?&lt;/h3>
&lt;p>While planning the migration, it is important to understand how your network is currently built and what are your ambitions for the migration project. This will help you identify potential issues and plan for a successful migration.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;span style="color:rgb(75, 194, 81);">Best Case Scenario&lt;/span>&lt;/th>
&lt;th>&lt;span style="color: #ff5733;">Worst Case Scenario&lt;/span>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Small Layer 2 (L2) subnets with only VMware assets.&lt;/td>
&lt;td>A flat network topology.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>All assets will migrate to the cloud.&lt;/td>
&lt;td>Not all resources will switch to the cloud&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>A good understanding of network dependencies between networks and assets.&lt;/td>
&lt;td>Limited knowledge of network dependencies.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>No dependencies with on-premises after migration.&lt;/td>
&lt;td>Lots of dependencies with on-premises after migration.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;div class="notices info">
&lt;div class="label">Note&lt;/div>
&lt;p>If we can easily determine best and worst case scenarios criteria, &lt;strong>all shades can exist&lt;/strong> between these two extreme scenarios.&lt;/p>
&lt;/div>
&lt;h2 id="risks-mitigation">Risks mitigation&lt;/h2>
&lt;p>In the following section, we will oversee some possible mitigation strategies for the previous risks.&lt;/p>
&lt;h3 id="networks-with-vmware-and-non-vmware-assets">Networks with VMware and non-VMware assets&lt;/h3>
&lt;ul>
&lt;li>Per network: consider the level of effort to re-IP one or the other category. Example:
&lt;ul>
&lt;li>&lt;em>80% of my assets will remain on-premises: change IP addresses for the 20% migrated?&lt;/em>&lt;/li>
&lt;li>&lt;em>80% of my assets will migrate to the cloud: change IP addresses for the 20% remaining on-premises?&lt;/em>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>L2 extension can still help to migrate resources, even if considering a re-IP strategy for the migrated workload.&lt;/li>
&lt;/ul>
&lt;h3 id="dependencies-with-on-premises-after-migration">Dependencies with on-premises after migration&lt;/h3>
&lt;ul>
&lt;li>Consider a cloud migration of services hosted on-premises: PaaS/IaaS etc.&lt;/li>
&lt;li>Adapt connectivity methods between environments: more bandwidth, less latency, etc.&lt;/li>
&lt;/ul>
&lt;h3 id="limited-knowledge-of-network-dependencies">Limited knowledge of network dependencies&lt;/h3>
&lt;ul>
&lt;li>&lt;em>Azure Migrate&lt;/em> can help to &lt;a href="https://learn.microsoft.com/en-us/azure/migrate/concepts-dependency-visualization">map all the dependencies of network and assets&lt;/a>.
&lt;ul>
&lt;li>+ &lt;a href="https://az-mdv.az.vupti.me/">Azure Migrate Network Flows Analysis&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Other tools like &lt;em>VMware Aria Operations for Networks&lt;/em>.&lt;/li>
&lt;/ul>
&lt;h2 id="vmware-hybrid-cloud-extension-hcx">VMware Hybrid Cloud Extension (HCX)&lt;/h2>
&lt;p>VMware HCX is a powerful tool that can help you extend your network and retain your IP addresses during the migration to Azure VMware Solution (AVS). It provides a seamless way to migrate workloads while maintaining their existing IP addresses, which can simplify the migration process and reduce downtime.&lt;/p>
&lt;div class="notices info">
&lt;div class="label">Note&lt;/div>
&lt;p>HCX Enterprise is a &lt;strong>free add-on for AVS&lt;/strong>: you can use it to migrate workloads from on-premises to AVS without any additional cost.&lt;/p>
&lt;/div>
&lt;h3 id="prerequisites-to-consider-for-hcx-l2-extensions">Prerequisites to consider for HCX L2 Extensions&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Prerequisites&lt;/th>
&lt;th>Mitigation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>(Standard) vSwitch are not supported by HCX to extend L2 network. &lt;br>→ Consider migrating to &lt;em>Distributed-vSwitch&lt;/em>&lt;/td>
&lt;td>&lt;ul>&lt;li>Easy to validate,&lt;/li>&lt;li>Relatively easy to remediate&lt;/li>&lt;/ul>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HCX support of NSX-V to NSX-T migration is &lt;a href="https://techdocs.broadcom.com/us/en/vmware-cis/hcx/vmware-hcx/4-11/hcx-4-11-release-notes/vmware-hcx-411-release-notes.html">deprecated in version 4.11&lt;/a>&lt;/td>
&lt;td>&lt;ul>&lt;li>Easy to validate,&lt;/li>&lt;li>Currently supported&lt;/li>&lt;/ul>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HCX support migration for vSphere and vCenter 6.5 &lt;a href="https://knowledge.broadcom.com/external/article?articleNumber=321571">with limited support&lt;/a>&lt;/td>
&lt;td>&lt;ul>&lt;li>Easy to validate,&lt;/li>&lt;li>Currently supported&lt;/li>&lt;/ul>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="traffic-tromboning-mitigation-with-hcx-mobility-optimized-network-mon">Traffic tromboning mitigation with HCX Mobility Optimized Network (MON)&lt;/h3>
&lt;p>One of the key benefits of using HCX is its ability to optimize network traffic and reduce latency. &lt;em>HCX Mobility Optimized Network (MON)&lt;/em> is a feature that helps to minimize the tromboning effect by optimizing the path that traffic takes between migrated and non-migrated assets or with resources in other networks.&lt;/p>
&lt;p>In a previous post (&lt;a href="https://vuptime.io/post/2023-08-17-hcx-to-the-mon-and-back/#mobility-optimized-network-enablement">VMware HCX: To the MON &amp;amp; Back&lt;/a>), we had the opportunity to see how HCX MON is working and how to configure it to greatly reduce the tromboning effect.&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Network path example with HCX L2 network extension and MON feature enabled"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario5-no-policy-routes.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;h2 id="nsx-autonomous-edge">NSX Autonomous Edge&lt;/h2>
&lt;p>An alternative approach to HCX is to use NSX Autonomous Edge (NSX AE) to extend your network and retain your IP addresses during the migration to Azure VMware Solution (AVS). This approach will rely on NSX-T VPN features to create tunnels between the on-premises and cloud environments, allowing you to extend your network and retain your IP addresses while migrating workloads.&lt;/p>
&lt;div class="notices info">
&lt;div class="label">Note&lt;/div>
&lt;p>Note: Standard vSwitch are supported with NSX AE.&lt;/p>
&lt;/div>
&lt;h3 id="prerequisites-and-limitations-to-consider-for-nsx-l2-extensions">Prerequisites and limitations to consider for NSX L2 Extensions&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Prerequisites&lt;/th>
&lt;th>Mitigation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Trunk interface required to extend multiple VLANs.&lt;br>&lt;ul>&lt;li>Promiscuous Mode required.&lt;/li>&lt;li>Forget Transmit required.&lt;/li>&lt;/ul>&lt;/td>
&lt;td>&lt;ul>&lt;li>Easy to validate,&lt;/li>&lt;li>Easy to remediate&lt;/li>&lt;/ul>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>No HCX MON-like optimization.&lt;/td>
&lt;td>Network extension cutover is recommended as soon as migration is completed.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Download NSX AE OVF requires Broadcom entitlement.&lt;/td>
&lt;td>n/a&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In conclusion, retaining your IP addresses while migrating to Azure VMware Solution (AVS) is not a complex process but requires &lt;strong>careful planning and consideration of network principles&lt;/strong>.&lt;/p>
&lt;p>By leveraging VMware Hybrid Cloud Extension (HCX) Layer 2 extensions or NSX Autonomous Edge, you can simplify the migration process and avoid/reduce downtime while maintaining existing IP addresses.&lt;/p></description></item><item><title>VMware HCX: To the MON &amp; Back</title><link>https://vuptime.io/post/2023-08-17-hcx-to-the-mon-and-back/</link><pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate><guid>https://vuptime.io/post/2023-08-17-hcx-to-the-mon-and-back/</guid><description>
&lt;p>No, we are not switching to a music blog genre to discuss the &lt;em>&amp;quot;To the Moon &amp;amp; Back&amp;quot;&lt;/em> song from &lt;strong>Savage Garden&lt;/strong> (I am sorry if you were expecting that) There is no typo in the title: we are going to explore VMware HCX network extensions and the MON feature, aka &lt;em>Mobility Optimized Network&lt;/em>.&lt;/p>
&lt;p>If you are not familiar with HCX, it is a VMware solution that allows you to migrate workloads from on-premises to the cloud, or from cloud to cloud. It also allows you to stretch your networks from migration source to the destination. It is a very powerful solution that can be used in many different scenarios to accelerate a migration project.&lt;/p>
&lt;p>In this article, we are going to focus on the network extension part of HCX, and more specifically on a poorly understood feature: &lt;em>Mobility Optimized Network&lt;/em>.&lt;/p>
&lt;h2 id="what-are-we-not-going-to-talk-about">What are we not going to talk about&lt;/h2>
&lt;p>In this post, I will no cover the creation of HCX network extensions in details. I assume that this subject is already well documented on Internet, including the official documentation and does not require a lot of explanations if you are already familiar with HCX.&lt;/p>
&lt;h2 id="lab-setup">Lab setup&lt;/h2>
&lt;p>In order to document this post, I created a lab based on the following topology:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Lab topology"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/lab-topology.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>In this lab we have:&lt;/p>
&lt;ul>
&lt;li>An on-premises-like environment with a vCenter and hypervisors, hosting:
&lt;ul>
&lt;li>A network (&lt;code>10.100.115.0/24&lt;/code>)&lt;/li>
&lt;li>A routing device (&lt;code>gw&lt;/code> @ &lt;code>10.100.115.1&lt;/code>) and its northbound connectivity (Internet + Cloud connectivity)&lt;/li>
&lt;li>A set of virtual machines to be migrated to the cloud: &lt;code>migration-vm-X&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>a cloud environment (Azure based) with:
&lt;ul>
&lt;li>Landing of the ExpressRoute circuit&lt;/li>
&lt;li>A point-to-site VPN gateway for my workstation&lt;/li>
&lt;li>A vNET: &lt;code>10.100.2.0/24&lt;/code>&lt;/li>
&lt;li>An Azure native VM on this vNET: &lt;code>azure-vm&lt;/code> @ &lt;code>10.100.2.36&lt;/code>&lt;/li>
&lt;li>An Azure VMware Solution SDDC with:
&lt;ul>
&lt;li>Express Route (ER) + Global Reach connectivity&lt;/li>
&lt;li>HCX Enterprise deployed and configured&lt;/li>
&lt;li>A native NSX-T segment with direct AVS connectivity with a test VM: &lt;code>10.100.110.0/24&lt;/code> and &lt;code>Ubuntu01&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>I extended the on-premises network to the cloud using HCX network extension in order to prepare the migration of the VMs. The extended network (&lt;code>10.100.115.0/24&lt;/code>) is now available in the cloud-side.&lt;/p>
&lt;h2 id="default-network-connectivity">Default network connectivity&lt;/h2>
&lt;p>Before we start migrating VMs, let's have a look at the default network connectivity on premises:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Default network connectivity on-premises"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario1-onpremises.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>&lt;code>migration-vm-X&lt;/code> is using the on-premises &lt;code>gw&lt;/code> device and its default route to reach the Internet &lt;span style="color:#FF0080;font-weight:bold;">(↔ pink)&lt;/span>. The &lt;code>gw&lt;/code> device is also used to reach the cloud environment, through the ExpressRoute circuit &lt;span style="color:#FF9933;font-weight:bold;">(↔ orange)&lt;/span>. To reach an Azure VMware Solution based VM, the ER circuit is used in addition with Global Reach and the AVS ER circuit &lt;span style="color:#FF2626;font-weight:bold;">(↔ red)&lt;/span>.&lt;/p>
&lt;h2 id="migrate-a-vm-to-the-cloud-environment">Migrate a VM to the cloud environment&lt;/h2>
&lt;p>Let's migrate &lt;code>migration-vm-2&lt;/code> to the cloud environment using HCX. The migration is successful and the VM is now running in the cloud environment. The VM is still using the on-premises &lt;code>gw&lt;/code> device to reach both the Internet and the cloud environment as its default gateway is still configured to &lt;code>10.100.115.1&lt;/code>.&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="migration-vm-2 still relies on its on-premises based gateway to reach resources out-of-its L2 broadcast domain"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario2-migrated-vm.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>To reach Internet &lt;span style="color:#FF0080;font-weight:bold;">(↔ pink)&lt;/span> or cloud based resources &lt;span style="color:#FF9933;font-weight:bold;">(↔ orange)&lt;/span>, the network path is not optimal and this is even more obvious when we look at path to reach VM in another NSX-T segment of AVS &lt;span style="color:#FF2626;font-weight:bold;">(↔ red)&lt;/span>. We call this situation: &lt;a href="https://en.wikipedia.org/wiki/Anti-tromboning">&lt;em>(w)&lt;/em> network tromboning&lt;/a>.&lt;/p>
&lt;h3 id="segment-connectivity">Segment connectivity&lt;/h3>
&lt;p>On NSX-T, when the network extension was created, a segment with the same subnet settings was created and named with &lt;code>L2E_&lt;/code> prefix.&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Segment connectivity of L2E network"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario2-nsx-t-connectivity.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>As you can see in the screenshot, this segment is configured with a &lt;strong>disabled&lt;/strong> &lt;code>gateway connectivity&lt;/code>. This means that the segment is not advertised to the other components of the NSX-T fabric cannot use the T1 gateway for L3 connectivity.&lt;/p>
&lt;h2 id="mobility-optimized-network-enablement">Mobility Optimized Network enablement&lt;/h2>
&lt;p>In order to improve the network path, we are going to enable the Mobility Optimized Network feature of HCX. This feature is available in the HCX UI, in the &lt;strong>Network Extension&lt;/strong> section, and can be enabled on a per-network basis.&lt;/p>
&lt;p>If we enable this feature, and not change the default settings, the connectivity of the segment is switched to &lt;strong>enabled&lt;/strong> and the T1 gateway &lt;strong>may&lt;/strong> now be used for L3 connectivity.&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Mobility Optimized Network enablement"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario3-default-mon.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>As you probably noticed, network paths are not changed for the migrated VM. This is due to the default setting for &lt;em>router-location&lt;/em>: &lt;code>hcx-enterprise&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>The &lt;em>router-location&lt;/em> &lt;code>hcx-enterprise&lt;/code> setting value means that on-premises &lt;code>gw&lt;/code> device is still used as the default gateway for the migrated VM.&lt;/p>&lt;/blockquote>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Router location for the migrated VM with default settings::picture-border"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario3-router-location.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;h3 id="segment-connectivity-1">Segment connectivity&lt;/h3>
&lt;p>Let's have a look at the segment connectivity after enabling the Mobility Optimized Network feature:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Segment connectivity of L2E network with MON enabled"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario3-nsx-t-connectivity.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>The &lt;code>gateway connectivity&lt;/code> is now &lt;strong>enabled&lt;/strong> on the L2E segment, and the T1 gateway could now used for L3 connectivity (depending on the &lt;em>router-location&lt;/em> setting).&lt;/p>
&lt;p>In BGP advertisement in the Express Route circuits, we can also see a new &lt;code>/32&lt;/code> route advertised from NSX-T:&lt;/p>
&lt;ul>
&lt;li>&lt;code>10.100.115.1/32&lt;/code>: the gateway of the extended network.&lt;/li>
&lt;/ul>
&lt;h2 id="changing-the-router-location">Changing the &lt;em>router-location&lt;/em>&lt;/h2>
&lt;p>To improve the network path for the migrated VM, we may be tempted to change the &lt;em>router-location&lt;/em> setting to use the cloud side gateway for our migrated VM:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Router location for the migrated VM with cloud side gateway::picture-border"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario4-cloud-router-location.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>The following changes will be applied to network path:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Network path when the router location is changed to cloud side gateway"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario4-cloud-router-location-network-flows.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;ul>
&lt;li>The default gateway configured at the the VM/OS is not changed: &lt;code>10.100.115.1&lt;/code> but...&lt;/li>
&lt;li>To reach a VM in a distinct NSX-T segment, the traffic will be routed through the T1 gateway of the segment, and not through the on-premises &lt;code>gw&lt;/code> device &lt;span style="color:#FF2626;font-weight:bold;">(↔ red)&lt;/span>.&lt;/li>
&lt;li>To reach the Internet, the traffic will be routed through the T1 gateway of the segment, and not through the on-premises &lt;code>gw&lt;/code> device &lt;span style="color:#FF0080;font-weight:bold;">(↔ pink)&lt;/span>.&lt;/li>
&lt;li>To reach a VM in the native cloud environment, the traffic will be routed through the on-premises &lt;code>gw&lt;/code> device &lt;span style="color:#FF9933;font-weight:bold;">(⇠ orange)&lt;/span>.
&lt;ul>
&lt;li>But the return path will be through the T1 gateway of the segment &lt;span style="color:#FF9933;font-weight:bold;">(⇠ orange)&lt;/span>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>As you can see there, if the network path to AVS hosted or Internet resources seems optimized, the path to native cloud resources is not and is asymmetric. This is because of a setting category in Mobility Optimized Network feature: &lt;strong>policy routes&lt;/strong>. We will explore this setting in the next sections.&lt;/p>
&lt;h3 id="what-happened-in-the-backstages">What happened in the backstages&lt;/h3>
&lt;p>When we changed the &lt;em>router-location&lt;/em> setting, the following change was applied:&lt;/p>
&lt;p>If we have a look at the routing table of the T1 gateway, a new entry was added for the migrated VM:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Routing table of the T1 gateway"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario3-static-routes.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Next hop of the static route for the migrated VM"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario3-static-route-next-hop.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>On the Express Route circuit, 2 new routes are also visible, advertised over BGP from NSX-T:&lt;/p>
&lt;ul>
&lt;li>&lt;code>10.100.115.1/32&lt;/code>: the gateway of the network is now advertised from AVS (&lt;em>this route was already advertised since the MON enablement&lt;/em>)&lt;/li>
&lt;li>&lt;code>10.100.115.12/32&lt;/code>: the migrated VM with MON enabled and &lt;em>router-location&lt;/em> set to HCX cloud instance.&lt;/li>
&lt;/ul>
&lt;h3 id="asymmetric-routing">Asymmetric routing&lt;/h3>
&lt;p>As you see on the &lt;a href="https://vuptime.io/post/2023-08-17-hcx-to-the-mon-and-back/#changing-the-router-location">network flow to a cloud based resource&lt;/a> (in a private network), there is an asymmetric routing. The traffic is routed through the on-premises &lt;code>gw&lt;/code> device to reach the cloud based resource, but the reverse path is going through the T1 gateway of the segment &lt;span style="color:#FF9933;font-weight:bold;">(⇠ orange)&lt;/span>, on cloud side.&lt;/p>
&lt;p>As NSX-T is now publishing the &lt;code>/32&lt;/code> route of the migrated VM, cloud resources can now reach the migrated VM directly through the T1 gateway of the segment. This is the reason why this, cloud resource to AVS one, path is through the T1 gateway.&lt;/p>
&lt;p>The reason of the &lt;code>migration-vm-X&lt;/code> to use the on-premises &lt;code>gw&lt;/code> device to reach the cloud based resource is because of the default &lt;strong>policy routes&lt;/strong> setup when MON is enabled:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Default policy routes when MON is enabled::picture-border"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario3-default-policy-routes.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>By default, the &lt;strong>policy routes&lt;/strong> are configured to be &lt;em>allowed&lt;/em> to use the on-premises &lt;code>gw&lt;/code> device as the default gateway for the traffic matching the RFC1918 address spaces:&lt;/p>
&lt;ul>
&lt;li>&lt;code>10.0.0.0/8&lt;/code>&lt;/li>
&lt;li>&lt;code>172.16.0.0/12&lt;/code>&lt;/li>
&lt;li>&lt;code>192.168.0.0/16&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>This enable the migrated VM to reach other resources of the on-premises network, via on-premises &lt;code>gw&lt;/code> device as the default gateway, but if not customized, it also introduces an asymmetric routing for the traffic to cloud based resources.&lt;/p>
&lt;h2 id="lets-customize-the-policy-routes">Let's customize the policy routes&lt;/h2>
&lt;h3 id="remove-all-policy-routes">Remove all policy routes&lt;/h3>
&lt;p>A good illustration to understand the impact of the policy routes is to do a test by removing all the pre-configured policy routes.&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Network path when there is no policy routes"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario5-no-policy-routes.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>For Internet &lt;span style="color:#FF0080;font-weight:bold;">(↔ pink)&lt;/span> or AVS based resources &lt;span style="color:#FF2626;font-weight:bold;">(↔ red)&lt;/span>, the network path is still the one from the previous section.&lt;/p>
&lt;p>For native cloud resources &lt;span style="color:#FF9933;font-weight:bold;">(↔ orange)&lt;/span>, the network path is now symmetric as the migrated VM is using the T1 gateway of the segment to reach all the resources out-of its L2 broadcast domain.&lt;/p>
&lt;p>&lt;strong>This setup could be sub-optimal&lt;/strong> for the migrated VM to reach on-premises resources, but this is something that can be customized by adding a new policy route with more specific matching criteria for the on-premises resources.&lt;/p>
&lt;h3 id="add-a-very-specific-policy-route">Add a very specific policy route&lt;/h3>
&lt;p>Another good illustration of how policy routes work in a MON enabled network extension is to add a very specific policy route to reach a specific resource with an optimal path.&lt;/p>
&lt;p>In our example, we will recreate the default policy routes and add a &lt;code>/32&lt;/code> one with a &lt;code>deny&lt;/code> rule, matching the Azure hosted resource &lt;code>azure-vm&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>10.0.0.0/8&lt;/code>: Send to source with HCX: &lt;span style="color:#00CC00;">allow&lt;span>&lt;/li>
&lt;li>&lt;code>172.16.0.0/12&lt;/code>: Send to source with HCX: &lt;span style="color:#00CC00;">allow&lt;span>&lt;/li>
&lt;li>&lt;code>192.168.0.0/16&lt;/code>: Send to source with HCX: &lt;span style="color:#00CC00;">allow&lt;span>&lt;/li>
&lt;li>&lt;code>10.100.2.36/32&lt;/code>: Send to source with HCX: &lt;span style="color:#FF2626;">deny&lt;span>&lt;/li>
&lt;/ul>
&lt;p>In this new setup, network path to the Azure hosted resource &lt;code>azure-vm&lt;/code> is now optimized in both directions:&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Network path when there is a very specific policy routes"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario6-specific-policy-route.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;ul>
&lt;li>To reach on premises resources in a private RFC1918 ranges (like in &lt;code>10.0.0.0/8&lt;/code>), the on-prem &lt;code>gw&lt;/code> device is used &lt;span style="color:#4D27AA;font-weight:bold;">(↔ purple)&lt;/span>.&lt;/li>
&lt;li>To reach a cloud based specific resource (&lt;code>10.100.2.36/32&lt;/code>), the cloud side gateway is used &lt;span style="color:#FF9933;font-weight:bold;">(↔ orange)&lt;/span>.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Note: I removed the internet connectivity to simplify the diagram but there is no change in the network path to reach Internet.&lt;/p>&lt;/blockquote>
&lt;h2 id="use-policy-routes-for-internet-connectivity">Use policy routes for internet connectivity&lt;/h2>
&lt;p>In the previous section, we saw that we can use policy routes to optimize the network path to reach a specific resource. We can also use policy routes to optimize or guide the network path to reach Internet (or &lt;code>0.0.0.0/0&lt;/code>).&lt;/p>
&lt;h3 id="internet-egress-with-default-policy-routes">Internet egress with default policy routes&lt;/h3>
&lt;p>Let's have a look at the network path to reach Internet with the default policy routes (&lt;em>router-location&lt;/em> is set to cloud side gateway):&lt;/p>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Network path to reach Internet with default policy routes and router-location set to cloud side gateway"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario7-internet-with-default-policy-routes.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>As Internet (&lt;code>0.0.0.0/0&lt;/code>) is not part of the RFC1918 address spaces configured to use the On-Prem gateway (with the default policy routes), the migrated VM is using the T1 gateway and the Azure egress connectivity of the segment to reach Internet &lt;span style="color:#FF0080;font-weight:bold;">(↔ pink)&lt;/span>.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Note&lt;/strong>: The azure egress path to reach Internet may vary depending on the configuration of the Azure VMware Solution SDDC. In this example, the Azure egress is configured to the default &lt;a href="https://vuptime.io/post/2022-08-12-azure-vmware-solution-public-ip-on-nsx-edge/#enable-outbound-internet-access-using-snat">&lt;em>Microsoft Managed SNAT&lt;/em>&lt;/a>.
You can find some details about the Internet connectivity for AVS, in the following post: &lt;a href="https://vuptime.io/post/2022-08-12-azure-vmware-solution-public-ip-on-nsx-edge/">Azure VMware Solution – Use public IP on NSX-T Edge&lt;/a>.&lt;/p>&lt;/blockquote>
&lt;h3 id="internet-egress-with-a-specific-policy-route">Internet egress with a specific policy route&lt;/h3>
&lt;p>Let's add a specific policy route to reach Internet through the on-premises &lt;code>gw&lt;/code> device (&lt;em>router-location&lt;/em> is still set to cloud side gateway):&lt;/p>
&lt;ul>
&lt;li>&lt;code>0.0.0.0/0&lt;/code>: Send to source with HCX: &lt;span style="color:#00CC00;">allow&lt;span>&lt;/li>
&lt;/ul>
&lt;p>&lt;figure>
&lt;picture>
&lt;img
loading="lazy"
decoding="async"
alt="Network path to reach Internet with a specific policy route and router-location set to cloud side gateway"
class="image_figure image_internal image_unprocessed"
src="https://vuptime.io/images/hcx-mon/scenario8-internet-with-specific-policy-route.png"
/>
&lt;/picture>
&lt;/figure>
&lt;/p>
&lt;p>With this new policy route, the migrated VM is now using the on-premises &lt;code>gw&lt;/code> device to reach Internet &lt;span style="color:#FF0080;font-weight:bold;">(↔ pink)&lt;/span>. You can then apply some firewall rules on the on-premises &lt;code>gw&lt;/code> device to control the Internet access of the migrated VM.&lt;/p>
&lt;p>Without additional policy routes, all the network flows will also use this on-premises &lt;code>gw&lt;/code> device: it could be counter-productive to enable MON in this case without adding additional policy routes to optimize the network path to reach other resources.&lt;/p>
&lt;h2 id="an-art-of-balance">An art-of-balance&lt;/h2>
&lt;div class="notices info">
&lt;div class="label">Disclaimer&lt;/div>
&lt;p>Do not reproduce the previous examples on a production environment.&lt;/p>
&lt;/div>
&lt;p>Previous examples are provided to illustrate the behavior of MON enabled resources and network flows based on settings changes. You will probably need to consider carefully how-to apply global and/or specific flows policies based on your deployment to avoid any issue and to maintain the expected level of security on the network flow path.&lt;/p>
&lt;p>For example, once a flow is using the NSX-T Tier1, it is not secured anymore by the on-premises firewall and may require to have some firewall rules setup on NSX-T level.&lt;/p>
&lt;p>Also, MON is coming with &lt;a href="https://docs.vmware.com/en/VMware-HCX/4.7/hcx-user-guide/GUID-BEC26054-D560-46D0-98B4-7FF09501F801.html">some limitations to consider&lt;/a> and may not be suitable for all the use cases. A good review of existing documentation is mandatory before proceeding in MON enablement. A good starting-point for AVS resources is the following documentation page: &lt;a href="https://learn.microsoft.com/en-us/azure/azure-vmware/vmware-hcx-mon-guidance">VMware HCX Mobility Optimized Networking (MON) guidance&lt;/a>.&lt;/p>
&lt;p>Finally, I will strongly suggest to consider network-extension &lt;em>cutover&lt;/em> operation as a critical step of your migration project and to plan it carefully. Mobility Optimized Networking feature is a great helper to optimize the network flow path, avoid or limit network tromboning scenario but should be considered as a tool to help you to achieve your migration goal and not as a magic feature that will solve all your network issues or provide a way to skip network extension cutovers operations. For long term network extensions, changing the default gateway of the migrated VM to the cloud side gateway may be a good option to optimize network flows.&lt;/p></description></item></channel></rss>