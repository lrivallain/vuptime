---
layout: post
title: VMworld 2019 ‚Äì R√©capitulatif jour 3
category: VMware
author: lrivallain
tags: vmware vmworld french
thumb: /images/vmworld2019.png
---

![Photo Booth avec des Toulousains](/images/vmworld2019/day3_VMWorldFest_2.gif)

* Sommaire
{:toc}

## Delivering Custom Services Through vCloud Director Extensibility `[HBI1855BE]`

* Auteur de la notice ci-dessous: *[Ludovic Rivallain](/about/#lrivallain)*
* Speaker(s):
  * **Nick de Kuijer**, Cloud Architect, Simac IT NL
  * **Martin Hosken**, Principal Architect, VMware
  * **Milko Slavov**, Staff Engineer, VMware
* [Vid√©o](https://videos.vmworld.com/global/2019/videoplayer/29337)

Je ne vais pas le cacher, c'√©tait la pr√©sentation que j'attendais le plus de ce VMworld! √âtant donn√© que je travaille sur le sujet de l'extensibilit√© de vCloud Director depuis plus d'un an (voir le post √† ce sujet [Extending VMware vCloud Director functionalities](/2019/05/06/extending-vcloud-director-functionalities/)), j'avais de hautes attentes concernant les m√©thodes utilis√©es par d'autres personnes pour r√©aliser le m√™me type d'extensions d'UI et d'API.

La pr√©sentation a commenc√© avec une description g√©n√©rale du concept d'extension des API et de l'UI du portail. Et de citer quelques exemples de use-case (notamment avec la tr√®s belle int√©gration r√©alis√©e par **Avamar**). Il y a aussi eu une annonce d'un nouveau framework permettant la cr√©ation d'int√©grations plus pouss√©es des extensions (notamment la possibilit√© d'ajouter des actions contextuelles sur des objets de l'inventaire) et plus facile √† mettre en ≈ìuvre.

Nous avons eu ensuite une d√©monstration un peu plus technique d'un *Backend as a Service* qui n'a pas √©t√© tr√®s clairement expliqu√©e √† mon avis.

Enfin, Nick de Kuijer, Cloud Architect @ [Simac IT](https://www.simac.com/en/itnl) üá≥üá± a d√©montr√© un exemple d'extension permettant la mise √† disposition des logs Firewall NSX dans le portail vCD, pour les utilisateurs. Leur permettant ainsi de pouvoir analyser si une requ√™te est l√©gitimement bloqu√©e ou pas. Puis un m√©canisme de d√©ploiement de VM personnalis√© permettant d'adapter la zone de disponibilit√©, le type d'instance‚Ä¶

## Evolving vRealize Automation `[HBO1323BE]`

* Auteur de la notice ci-dessous: *[Ludovic Rivallain](/about/#lrivallain)*
* Speaker(s):
  * **Chris McClanahan**, Staff Technical Marketing Architect, VMware
  * **Liad Ofek**, Director, Product Managerment, VMware
* [Vid√©o](https://videos.vmworld.com/global/2019/videoplayer/29414)

Note √† moi-m√™me pour l'ann√©e prochaine: ne pas aller √† des conf√©rences de niveau technique 100 ou 200. Jamais. M√™me quand le sujet semble passionant.

C'√©tait le cas de cette pr√©sentation de vRealize Automation 8.0. J'en attendais beaucoup car vRA est un produit tr√®s utilis√© par nos clients et sur lequel nous passons nous-m√™me beaucoup de temps pour y r√©aliser de l'automatisation. Et comme le changement 7.x > 8.0 s'annonce majeur, il vaut mieux se renseigner au maximum au plus t√¥t.

Pour r√©sumer tout de m√™me les informations majeures:

* vRA a √©t√© totalement r√©√©crit pour r√©duire sa complexit√© de d√©ploiement. Les services IaaS ont notamment disparu.
* L'installation et la maintenance de vRA s'appuyent √† pr√©sent sur Life Cycle Manager et la gestion d'identit√© sur le Identity Manager.
* Tous les services vRA sont √† pr√©sent devenus des services bas√©s sur Kubernetes.
  * J'ai tout de m√™me un doute sur les services PostgreSQL et RabbitMQ car la slide n'√©tait pas limpide du tout.
* L'apparition de *FaaS* (Function as a Service) comme nouveau support √† l'extensibilit√©, au cot√© de vRO. Par exemple AWS Lambda mais pas que.
* Une int√©gration git compl√®te pour les √©l√©ments du catalogue de service!

VMware indique clairement que cette version 8.0 est une version qui n'est pas √† mettre en production car elle manque de maturit√©. Il faudra donc attendre au moins la version 8.1, premier semestre 2020. La migration de quelques donn√©es de plateformes 7.5 ou 7.5 ne sera elle disponible qu'√† partir de la mi-2020. C'est un gros gros challenge pour nos clients qui avaient mis√©s sur l'extensibilit√© de vRA 7.x et ne pourront probablement pas r√©utiliser beaucoup de ces efforts en vRA 8.x‚Ä¶

## The Art of Code That Writes Code `[CODE2216E]`

* Auteur de la notice ci-dessous: *[Ludovic Rivallain](/about/#lrivallain)*
* Speaker(s):
  * **Kyle Ruddy**, Senior Technical Marketing Architect, VMware
* [Vid√©o](https://videos.vmworld.com/global/2019/videoplayer/29883)

Qui ne r√™ve pas d'un outil permettant de g√©n√©rer d'autres outils du m√™me type tout en √©crivant le moins de code possible. Vous d√©crivez une nouvelle API, c'est d√©j√† un beau travail. Et vous devez en plus coder des clients ou des SDK pour utiliser cette API en ligne de commande par exemple? C'est un doublon de travail qui peut √™tre √©vit√©.

On recommence: Vous d√©crivez une nouvelle API en la documentant au fur et √† mesure. Par exemple en r√©alisant un fichier de description *swagger*. Ensuite, il suffit de faire dig√©rer ce descripteur √† un outil comme [`autorest`](https://github.com/Azure/autorest) (Azure) pour obtenir un client ou un SDK en C#, NodeJS, Python, Java, Ruby, Go, TypeScript‚Ä¶

Kyle nous a propos√© un exemple bas√© sur l'API [xkcd](https://xkcd.com/json.html) et la g√©n√©ration d'un client en PowerShell. Exemple: `Get-XkcdComic`.

L'occasion de rappeler que les API vSphere √©voluent dans le bon sens (c'est √† dire vers plus de REST, moins de SOAP!) et que VMware propose nombre de SDK dans diff√©rents languages: [vsphere-automation-sdk](https://vmware.github.io/vsphere-automation-sdk/).


## Kubernetes Operators for VMware Enterprise PKS and VMware Cloud PKS `[CODE1360E]`

* Auteur de la notice ci-dessous: *Christian Tritten*
* Speaker(s):
  * **Michael Gasch**, Application Platforms Architect - Office of the CTO, VMware
  * **Tom Schwaller**, Technical Product Line Manager, CNABU, VMware
* [Vid√©o](https://videos.vmworld.com/global/2019/videoplayer/29564)

Plong√©e en d√©tail sur ce qu'est un *Operator*, √† quoi √ßa peut servir et comment en √©crire. Les _Operators_ coupl√©s aux _Custom Resource Definitions_ sont un moyen extr√™mement puissant d'√©tendre l'api de Kubernetes pour introduire des logiques m√©tiers complexes :

* Les __Custom Resource Definitions__ permettent de d√©clarer de nouveaux types de Ressources que Kubernetes va pouvoir g√©rer de la m√™me mani√®re que ses Ressources natives. Une fois la CRD d√©clar√©e, il devient possible de cr√©er des objets (Custom Resources) de ce nouveau type. La validation des Customs Resources cr√©√©es par les utilisateurs se base sur [la sp√©cification OpenAPI 3.0](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md).
* Les __Operators__ sont des Controllers Kubernetes (du code ex√©cut√© dans des Pods) qui surveillent les √©v√®nements li√©s aux Custom Resources et impl√©mentent la logique m√©tier pour r√©aliser des t√¢ches complexes (comme par exemple le d√©ploiement d'un syst√®me de base de donn√©es en cluster, la gestion de la sauvegarde des base ou encore l'ajout de nouveaux noeuds au cluster).

Il existe plusieurs frameworks pour faciliter le d√©veloppement d'Operators.

La d√©mo montre comment utiliser le framework [Kopf](https://github.com/zalando-incubator/kopf), d√©velopp√© par Zalando, pour √©crire un Operator qui va cr√©er des VMs sur vSphere via des CR de type VMGroup.

## Project Pacific: Guest Clusters Deep Dive `[HBI4500BE]`

* Auteur de la notice ci-dessous: *Christian Tritten*
* Speaker(s):
  * **Derek Beard**, Senior Staff Engineer, VMware
  * **Zach Shepherd**, Staff Engineer II, VMware
* [Vid√©o](https://videos.vmworld.com/global/2019/videoplayer/29784)

Cette session expose en d√©tail le projet Pacific.

> Kubernetes as a platform platform  
> -- <cite>Joe Beda</cite>

> Project Pacific is a re-architecture of vSphere with Kubernetes as its control plane.  
> -- <cite>Jared Rosoff</cite>

Kubernetes devient un √©l√©ment central de vSphere via l'introduction d'un composant d'orchestration nomm√© _Kubernetes Cluster Supervisor_. Le Supervisor utilise ESXi comme worker nodes au lieu de Linux. Ceci passe par une impl√©mentation sp√©cifique de Kubelet, nomm√©e __Spherelet__ qui tourne directement sur ESXi. Il devient ainsi possible de faire tourner des Pods kubernetes natifs sur ESXi avec des performances impressionnantes (30% plus rapide que des Pods tournant dans des VMs Linux, et 8% plus rapide que des Pods tournant sur du bare metal Linux, ceci gr√¢ce √† des optimisations sur la gestion de la m√©moire et du cpu).

D'autre part, le Supervisor se base sur des _CRDs_ et _Operators_ sp√©cifiques (Machine, MachineSet, MachineDeployment, Cluster, ManagedCluster) afin de permettre de g√©rer des objets vSphere via des fichiers de description de ressources au format Kubernetes. Des Operators d√©di√©s comme le _VM operator_ ou le _Guest Cluster Operator_ offrent la possibilit√© d'orchestrer le d√©ploiement et cycle de vie de :

* Machines Virtuelles,
* Clusters Kubernetes Manag√©s (dit 'Guest clusters').

Il devient ainsi plus facile de s√©parer les pr√©occupations entre les Ops qui d√©ploient l'infrastructure et les d√©veloppeurs qui vont acc√©der √† des VMs et des clusters Kubernetes √† la demande via des ressources de type Machine, ou ManagedCluster.

## The Circle of (Token) Life `[CODE3332E]`

* Auteur de la notice ci-dessous: *Christian Tritten*
* Speaker(s):
  * **Dan Illson**, Staff Native Cloud Advocate, VMware
* [Vid√©o US](https://videos.vmworld.com/global/2019/videoplayer/28397)

Dan Illson pr√©sente la gestion de donn√©es sensibles dans Kubernetes avec Hashicorp Vault. Il rappelle que les Secrets dans Kubernetes sont simplement encod√©s en base64 mais pas du tout chiffr√©s ce qui pose des probl√®mes en mati√®re de s√©curit√©. De plus l'acc√®s aux secrets manque cruellement de granularit√©.

L'outil Vault vient compl√©ter la gestion des secrets avec tout un panel de fonctionnalit√©s comme le chiffrement, la rotation des cl√©s, l'expiration ou la r√©vocation d'un secret, secrets dynamiques, etc...).

## NSX-T Deep Dive: Kubernetes Networking `[CNET1270BE]`

* Auteur de la notice ci-dessous: *Christian Tritten*
* Speaker(s):
  * **Ali Al Idrees**, Lead EMEA SDDC Architect, VMware
  * **Yasen Simeonov**, Senior Technical Product Manager, VMware
* [Vid√©o](https://videos.vmworld.com/global/2019/videoplayer/29713)

Apr√®s une br√®ve pr√©sentation de Kubernetes, cette session expose comment l'int√©gration du r√©seau NSX-T avec Kubernetes permet de g√©rer l'ensemble des probl√©matiques r√©seau (adressage des Pods, cloisonnement entre namespaces, loadbalancing L4 et L7, firewalling) via les ressources Kubernetes habituelles (ns, networkpolicies, ingresses).

Le composant NSX Container Plugin (NCP) est charg√© d'assurer la traduction (ainsi que la synchronisation) des r√®gles d√©finies dans les objets Kubernetes vers les r√®gles NSX-T correspondantes. Le composant NCP se pr√©sente sous la forme d'un Pod qui est ex√©cut√© directement dans le cluster Kubernetes.

## Service Mesh, Tracing, Prometheus: Wavefront Provides Observability for All `[KUB1862BE]`

* Auteur de la notice ci-dessous: *Christian Tritten*
* Speaker(s):
  * **Chhavi Nijhawan**, Product Line Marketing Manager, VMware
  * **Nikolay Nikolaev**, Open Source Networking Team Lead, VMware
* [Vid√©o](https://videos.vmworld.com/global/2019/videoplayer/29837)

Cette session commence par une introduction aux concepts de service mesh, de distributed tracing et de monitoring dans Kubernetes. Ces outils se r√©v√®lent utiles lorsqu'on l'on commence √† d√©ployer des applications fortement orient√©es microservices.

On pr√©sente ensuite l'outil Wavefront, et comment celui-ci peut venir s'interfacer avec les outils opensources istio, jaeger/zipkin, et prometheus. Wavefront se propose d'enrichir les fonctionnalit√©s de ces outils mais aussi de permettre leur utilisation dans une interface unifi√©e. Wavefront fourni en outre plus de 200 int√©grations pour ing√©rer des m√©triques et des logs de toute provenance.

La d√©mo pr√©sente un cas d'usage d'observabilit√© en d√©montrant comment rep√©rer d'o√π provient une latence dans une application microservices via la combinaison du monitoring pour la d√©tection du probl√®me, du tracing pour l'analyse d'une requ√™te de bout en bout et de l'analyse des logs pour trouver la cause pr√©cise du probl√®me. Tout cela sans quitter l'interface web de Wavefront.

## NSX-T Deep Dive : L3 Routing `[CNET1069BE]`

* Auteur de la notice ci-dessous: *J√©r√©my Rossignol*
* Speaker(s):
  * **Amit Aneja**, Senior Technical Product Manager, VMware
* [Vid√©o](https://videos.vmworld.com/global/2019/videoplayer/29591)

Ici nous avons eu un cours acc√©l√©r√© sur le rroutage au sein de NSX-T.
Petit rappel, toujours pour commencer un logical router NSX-T est constitu√© de deux composants le DR et le SR.

Le DR ou *Distributed Router* s'√©xecute localement sur les transport nodes. Une transport node cela peut √™tre √† la fois un ESXi ou un hyperviseur KVM qui h√©bergeront vos VMs clientes mais cela peut √™tre aussi une edge node qui elle h√©bergera vos logical router. Le SR ou *Service Router* lui s'√©xecutera seulement sur les *edge nodes*.

Le DR comme son nom l'indique est distribu√© sur votre plateforme et permet de faire tout le travail de routage. Le SR quant √† lui va se d√©ployer automatiquement lorsque vous aurez configur√© un service sur l'un de LR que ce soit *T0* ou *T1*, il n'est pas distribu√© mais centralis√©.

Voil√† si on r√©capitule d√©j√† ce petit rappel qui a lui tout seul vous donne certainement d√©j√† des cheveux blancs :

* NSX-T c'est du tier routing avec des routeurs *T0* et *T1*, *T0* permet de se connecter au r√©seau physique et *T1* est fait pour se connecter √† vos workloads
* Un *logical router* *T0* ou *T1* est compos√© d'un *DR* ET d'un *SR* si on configure un service dessus (Connexion au physique, routage dynamique, *LB, VPN, NAT, Firewall,DHCP, DNS Forwarder, Metadata Proxy*)
* *DR* : routage distribu√©
* *SR* : service, centralis√©

Ensuite maintenant que vous avez les bases, parlons un peu des *edges nodes* qui dans NSX-T peuvent √™tre √† deux formats, *baremetal* ou VM. ces edges nous allons donc les d√©ployer gr√¢ce √† un ova sur des ESXi ou directement sur un serveur *baremetal*.
L'utilit√© de la *edge node*, c'est d'h√©berger vos logical router *T0* et *T1* qui ne sont plus des VMs au sens propre mais plut√¥t des micro-services au sein de votre *edge node*.

Parlons maintenant un peu de routage, nous pouvons faire de l'*Equal Cost MultiPathing* avec NSX-T et notament les T0, par contre si vous voulez faire de l'ECMP, il faudra imp√©rativement utiliser le *Bidirectional Forwarding Detection* (BFD) en BGP.
Si vous √™tes dans un cas ou vous avez plusieurs *T0* en HA active/active alors dans ce cas les √©changes de routes entre vos deux *SR* *T0* se feront via iBGP.
Chose importante, le routage dans NSX-T se fera toujours au plus proche de l'√©metteur, si vous avez une VM qui √©met une trame qui a besoin d'√™tre rout√©e, le routage se fera toujours sur le *DR* de l'ESXi qui h√©berge la VM.

Prenez en consid√©ration que vos *uplinks* sur vos *T0* serviront toujours √† se connecter au r√©seau physique et que vos *T1* auront toujours, ou du moins quand ils sont connect√©s au *T0*, une route par d√©faut vers votre *T0*.

Bon alors tout cela c'est tr√®s technique, je vous recommande comme Amit d'ailleurs d'aller voir les *Design Guides*.

Petite nouveaut√© de la version 2.5, les *failure domains* pour les *edge clusters* (ensemble de *edge nodes*), vous pouvez maintenant d√©finir au sein d'un m√™me cluster edge 2 *failure domains* qui repr√©senteront par exemple deux racks diff√©rents dans votre datacenter, ce qui permettra lorsque vous utilisez du HA d'avoir vos instances *T0* sur diff√©rents racks en cas de panne cela peut √™tre utile  :)
Je parle ici seulement de *T0* car il n'ya que les *T0* qui peuvent √™tre en mode active/active.

Enfin le plus possible il faut installer vos services au plus proche du workload, c'est √† dire le plus possible sur vos *T1* pour permettre d'avoir vos *T0* en mode active/active car vous ne pouvez pas faire de active/active sur vos *T0* si vous installez des services dessus.

Si vous voulez aller plus loin voici les conf√©rences que vous pourrez aller voir:

* Network Virtualization and NSX-T - A technical Overview `[CNET1582BE]`
* NSX-T Deep Dive: Logical Switching `[CNET1511BE]`
* NSX-T Deep Dive: Load balancing `[CNET1356BE]`
* NSX-T Deep Dive: Connecting Cloud and Data Centers via NSX-T VPN `[CNET2841BE]`
* Apply Consistent Security Across VMs, Containers, and Bare Metal `[SAI1017BE]`

## Solutions Exchange

* Auteur de la notice ci-dessous: *J√©r√©my Rossignol*

Cette ann√©e j'ai voulu prendre le temps d'aller faire un tour et discuter au *Solutions Exchange* sur les diff√©rents stands des √©diteurs.

Voici quelques technos auquelles vous devriez penser si vous cherchez de nouvelles solutions, √©videmment ce n'est que mon avis et je invite √† vous faire votre propre avis sur ces technos :

* **AVI Networks** : appliance virtuelles de load balancing concurrente de F5 et rachet√© tout r√©cement par VMware
* **Rubriq** : j'ai eu l'occasion d'avoir une petite d√©mo du produit avec une discussion technique tr√®s int√©ressante, pensez-y si vous souhaitez changer votre solution de sauvegarde.
* **Datacore** : vous souhaitez faire du stockage hyperconverg√©, franchement j'y allais un peu √† reculons mais finalement ce produit a l'air d'√™tre une alternative √† VSAN tr√®s int√©ressante, en plus elle vous permettrait de r√©utiliser vos anciennes baies de stockage.
* **Cohesity** : de la m√™me mani√®re que Rubriq, pour moi ces 2 √©diteurs sont en train de faire bouger le monde de la sauvegarde, ce sont vraiment des solutions √† prendre en compte lors du choix de votre future solution de sauvegarde.

![Stereophonics au VMworld Fest](/images/vmworld2019/day3_VMWorldFest_1.png)